{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2.2: Validation in medical image analysis (Segmentation)\n",
    "\n",
    "This notebook combines theory with questions to support the understanding of validation metrics in medical image analysis. Use available markdown sections to fill in your answers to questions as you proceed through the notebook.\n",
    "\n",
    "**Contents:** <br>\n",
    "\n",
    "1. [Validation (concepts)](#validation)<br>\n",
    "\n",
    "    1.1 [Quality characteristics](#quality_characteristics)<br>\n",
    "    \n",
    "    - [Accuracy (bias)](#accuracy)<br>\n",
    "    - [Precision (variation), reproducibility, reliability, replicability](#precision)<br>\n",
    "    - [Robustness](#robustness)<br>\n",
    "    - [Efficiency](#efficiency)<br>\n",
    "    - [Fault detection](#fault)<br>\n",
    "    \n",
    "    1.3 [Measures of quality](#quality_measures)<br>\n",
    "    \n",
    "    - [Segmentation - quality measures](#seg_qm)<br>\n",
    "    \n",
    "    \n",
    "2. [Common limitations of performance metrics in biomedical image analysis](#limitations)\n",
    "\n",
    "    - [Small structures](#small_structures)<br>\n",
    "    - [Image artifacts](#artifacts)<br>\n",
    "    - [Overlap measurements](#overlap_measurements)<br>\n",
    "    - [Over- and undersegmentation](#over_underseg)<br>\n",
    "    - [Single-object bias](#object_bias)<br>\n",
    "    - [Metric combination](#combination)<br>\n",
    "    - [Choosing the right metric for a given task](#right_metric)<br>\n",
    "\n",
    "\n",
    "**References:** <br>\n",
    "\n",
    "[1] Measures of quality: [Toennies Klaus, D. Guide to Medical Image Analysis - Methods and Algorithms, Chapter 13.1](https://link.springer.com/book/10.1007/978-1-4471-2751-2)\n",
    "\n",
    "[2] Ground truth: [Toennies Klaus, D. Guide to Medical Image Analysis - Methods and Algorithms, Chapter 13.2](https://link.springer.com/book/10.1007/978-1-4471-2751-2)\n",
    "\n",
    "[3] Limitations of performance metrics: [Reinke et al. Common Limitations of Image Processing Metrics: A Picture Story.](https://arxiv.org/abs/2104.05642)\n",
    "\n",
    "[4] Assessment of registration errors: [Fitzpatrick, M. Visualization, Image-Guided Procedures and Modeling, 7261:1–12, SPIE Medical Imaging (2009).](https://spie.org/Publications/Proceedings/Paper/10.1117/12.813601)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='validation'></div>\n",
    "\n",
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/read_ico.png\" width=\"42\" height=\"42\"></div> \n",
    "\n",
    "## 1. Validation (concepts)\n",
    "\n",
    "Validation of medical image analysis methods is the estimation of correctness of certain results from tests of a method on a representative sample set. In e.g. software design, validation is the evaluation of the degree to which user needs (performance requirements) are met, i.e. whether the right software is being built. In medical image analysis, we usually talk about technical validation, where the aim is to evaluate the performance of computing algorithms with respect to e.g. segmentation accuracy. Validation is used in various image computing classes (registration, segmentation, detection, classification, quantification). \n",
    "\n",
    "Prior to performing validation, suitable data needs to be selected, comparison measures need to be chosen, and a norm (e.g. [ground-truth](#ground_truth), explained below) needs to be defined. **Remember that every validation study must have a hypothesis on performance (e.g. outcome is better than...), and a ground truth (gold standard) is essential**. Validation can provide information about our method with respect to another method used to generate the same results (cross-validation). It is mandatory to document a detailed description of the validation procedure together with a well-founded justification of selected measures, as it allows potential new users of the method to investigate the validity of the arguments used to build the validation scenario.\n",
    "\n",
    "<div id='quality_characteristics'></div>\n",
    "\n",
    "### 1.1 Quality characteristics\n",
    "There are a number of quality characteristics used in validation of medical image analysis methods: \n",
    "\n",
    "<div id='accuracy'></div>\n",
    "\n",
    "#### Accuracy (bias)\n",
    "\n",
    "Accuracy determines the deviation of results from known ground truth. It is computed via a measure of quality ([section 1.3](#quality_measures)) comparing results with some norm. Accuracy is calculated as the ratio between true/false positives and negatives: $\\mathrm{A} = \\frac{(\\mathrm{TP + TN})}{(\\mathrm{TP+FP+FN+TN})}$.\n",
    "\n",
    "<div id='precision'></div>\n",
    "\n",
    "#### Precision (variation), reproducibility, reliability, replicability \n",
    "\n",
    "These characteristics measure the extent to which equal or similar input produces equal or similar results. Reliable methods produce output within a given range of variation (e.g. in terms appearance). A method is reproducible, if two runs of this method with the exact same input and setup produce the exact same results. Replicability of a method can be determined if two runs of a method with the same input and same setup arrive at similar conclusions.\n",
    "<br>\n",
    "<br>\n",
    "<center width=\"100%\"><img src=\"../reader/assets/accuracy_precision.png\" width=\"600\"></center>\n",
    "\n",
    "<div id='robustness'></div>\n",
    "\n",
    "#### Robustness \n",
    "\n",
    "Robustness of a method characterizes the change of the quality of an analysis result if conditions deviate from assumptions made for analysis (e.g., when noise level increases or if object appearance deviates from prior assumptions). For example, robustness of a segmentation algorithm is the ability of an algorithm to persist in sufficient performance despite abnormalities in the input images (e.g. due to patient motion). Reproducibility and robustness are also important performance indicators in case of varying image data (different scanners, hospitals, patient populations, etc.).\n",
    "\n",
    "<div id='efficiency'></div>\n",
    "\n",
    "#### Efficiency\n",
    "\n",
    "The effort which must be exerted to achieve an analysis result is described by efficiency. You may recall that there are semi-automated methods that require some degree of human interaction or expert knowledge. These factors contribute to the overall determination of a method's efficiency.\n",
    "\n",
    "<div id='fault'></div>\n",
    "\n",
    "#### Fault detection\n",
    "\n",
    "The ability to discover possible faults while an analysis method is being applied is called fault detection. It is a very useful feature, because it requires the method to test for reliability of its own results. \n",
    "\n",
    "\n",
    "\n",
    "<div id='quality_measures'></div>\n",
    "\n",
    "### 1.3 Measures of quality\n",
    "\n",
    "Quality is determined by the kind of analysis which has been conducted on a dataset:\n",
    "\n",
    "| Task         | Quality measure                                                           |\n",
    "|:--------------|:---------------------------------------------------------------------------:|\n",
    "| Segmentation  | Correspondence between the segmented object and a reference segmentation |\n",
    "| Registration | Deviation from the correct registration transformation                    |\n",
    "| Computer-aided detection (CAD)    | Ratio between correct and incorrect decisions                             |\n",
    "\n",
    "See also [chapter 13.1 of the Guide to Medical Image Analysis by Tonnies, Klaus D](https://www.springer.com/gp/book/9781447160960)\n",
    "\n",
    "<div id='seg_qm'></div>\n",
    "\n",
    "#### Segmentation - quality measures\n",
    "\n",
    "When segmenting an object in an image, a measure of comparison between some reference $g$ (usually a ground truth) and the segmented object $f$ is required. Mutual correspondence may be determined by calculating volumetric overlap, overlap between object and background or performing distance measurements (of boundary deviations). In 3D cases, volumetric measurements aim to count the number of voxels in both the segmented object and the reference norm weighted by the volume covered by each voxel. \n",
    "\n",
    "Overlaps between objects $f$ and $g$ can be calculated by measures that count over-segmentation (number of elements) and under-segmentation. \n",
    "\n",
    "<div id=\"dsc_hd_iou\"></div>\n",
    "\n",
    "The next measures often used for quality assessment are _Dice similarity coefficient_ (DSC) a.k.a _Sørensen–Dice coefficient_ ($d$),  _Intersection over union_ ($i$) and the _Jaccard index_ ($j$):\n",
    "\n",
    "\\begin{equation}\n",
    "d = \\frac{2|F\\cap\\,G|}{|F|+|G|}\\,\\,, \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "i = \\frac{\\mathrm{DSC}}{2 - \\mathrm{DSC}}\\,\\,, \\mathrm{and}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "j = \\frac{|F\\cap\\,G|}{|F\\cup\\,G|}\\,\\,,\n",
    "\\end{equation}\n",
    "\n",
    "where $F\\cap G$ is the size of elements (voxels) in overlap, and $|F|$, $|G|$ are the sizes of individual volumes. The coefficient is equal to 1 in case of perfect correspondence; otherwise it is smaller than 1. In the medical image analysis community, the Dice coefficient is more popular, and therefore also more often present in literature.\n",
    "\n",
    "Neither Dice nor Jaccard indices can be used to measure outliers (e.g. in tasks where organ boundaries are to be segmented as part of access planning in surgery). In minimally invasive procedures, it is crucial to determine the deviation of the segmented boundary from the true boundary. This can be done by _Hausdorff distance_ (HD) between $F$ and $G$. The Hausdorff distance is defined as the maximum of all shortest distances $d$ between points in $F$ and $G$. Since this measure is highly sensitive to image artefacts, the quantile Hausdorff distance is used, where distances of largest outliers are averaged. It is computed from a quantile of a histogram of distances from $F$ to $G$ and from $G$ to $F$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h^{q} = \\mathrm{max}(t_{q}(d(f,G)),t_{q}(d(g,F)))\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='limitations'></div>\n",
    "\n",
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/read_ico.png\" width=\"42\" height=\"42\"></div> \n",
    "\n",
    "## 2. Common limitations of performance metrics used for segmentation tasks\n",
    "\n",
    "Recent meta-analytical research has detected major issues in algorithm validation. Most of these flaws are related to the practical use of some performance metrics in a given analysis task. One of the core issues in medical image analysis is the choice of inappropriate metrics [Maier-Hein, L. et al. (2018)](https://www.nature.com/articles/s41467-018-07619-7). In the same publication, it has been reported that image segmentation is the most popular of all medical image processing tasks taking into account international challenges. In these competitions, the chosen metrics significantly influence the rankings of various methods, and it was found out that researchers are missing guidelines for choosing the right metric for a given problem. More information can be found in the article [Common Limitations of Image Processing\n",
    "Metrics: A Picture Story](https://arxiv.org/abs/2104.05642).\n",
    "<br>\n",
    "\n",
    "<div id='small_structures'></div>\n",
    "\n",
    "### Small structures\n",
    "\n",
    "It is important to understand the mathematical properties of a metric before applying it to a given task. Segmentation of small structures, such as brain lesions (e.g. multiple sclerosis) often employs Dice scores, which may not be an appropriate metric because of the often unknown pathological outlines and high inter-observer variability in such tasks. The predictions of two algorithms may differ only by one pixel, yet the impact on the Dice score outcome is substantial (see figure below).\n",
    "<br>\n",
    "<br>\n",
    "<center width=\"100%\"><img src=\"../reader/assets/small_structure_segmentation.jpg\" width=\"600\"></center>\n",
    "\n",
    "<font size=\"1\">Figure from [Common Limitations of Image Processing\n",
    "Metrics: A Picture Story](https://arxiv.org/abs/2104.05642)</font>\n",
    "<br>\n",
    "\n",
    "<div id='artifacts'></div>\n",
    "\n",
    "### Image artifacts\n",
    "\n",
    "Similar issues may arise in the presence of image artifacts such as noise or errors in reference annotations. As seen in the figure below, a single erroneous pixel in the reference annotation may lead to a large performance decrease.\n",
    "<br>\n",
    "<br>\n",
    "<center width=\"100%\"><img src=\"../reader/assets/noise_effect_segmentation.jpg\" width=\"400\"></center>\n",
    "\n",
    "<font size=\"1\">Figure from [Common Limitations of Image Processing\n",
    "Metrics: A Picture Story](https://arxiv.org/abs/2104.05642)</font>\n",
    "<br>\n",
    "\n",
    "<div id='overlap_measurements'></div>\n",
    "\n",
    "### Overlap measurements\n",
    "\n",
    "In overlap measurements, dedicated metrics are incapable of discovering differences in shapes, which may have huge impact e.g. on radiotherapy applications. Completely different predictions may therefore lead to the exact same DSC value.\n",
    "<br>\n",
    "<br>\n",
    "<center width=\"100%\"><img src=\"../reader/assets/shape_unawareness.jpg\" width=\"500\"></center>\n",
    "\n",
    "<font size=\"1\">Figure from [Common Limitations of Image Processing\n",
    "Metrics: A Picture Story](https://arxiv.org/abs/2104.05642)</font>\n",
    "<br>\n",
    "\n",
    "<div id='over_underseg'></div>\n",
    "\n",
    "### Over- and undersegmentation\n",
    "\n",
    "In some applications detecting over- and undersegmentation, the DSC metric does not represent these performance indicators reliably, while HD is invariant to these properties.\n",
    "<br>\n",
    "<br>\n",
    "<center width=\"100%\"><img src=\"../reader/assets/over_under_segmentation.jpg\" width=\"500\"></center>\n",
    "\n",
    "<font size=\"1\">Figure from [Common Limitations of Image Processing\n",
    "Metrics: A Picture Story](https://arxiv.org/abs/2104.05642)</font>\n",
    "<br>\n",
    "\n",
    "<div id='object_bias'></div>\n",
    "\n",
    "#### Single object bias\n",
    "\n",
    "Commonly, segmentation metrics, such as DSC, are applied to detection and localization problems as well. In general, the DSC tends to be strongly biased against single objects, which is why its application in detection tasks should be avoided. An example where DSC underperforms, can be seen below.\n",
    "<br>\n",
    "<br>\n",
    "<center width=\"100%\"><img src=\"../reader/assets/detection_performance.jpg\" width=\"500\"></center>\n",
    "\n",
    "<font size=\"1\">Figure from [Common Limitations of Image Processing\n",
    "Metrics: A Picture Story](https://arxiv.org/abs/2104.05642)</font>\n",
    "<br>\n",
    "\n",
    "<div id='combination'></div>\n",
    "\n",
    "#### Metric combination\n",
    "\n",
    "Metrics are typically combined over all test cases to produce overall ranking. However, this can be detrimental in case of missing values (NA) and lead to a substantially higher DSC or varying HD compared to setting missing values to zero. Moreover, a single metric usually does not reflect all important features for algorithm validation. Through the combination of multiple metrics helps mitigate the problem, it has to be kept in mind that some metrics are mathematically related to each other, such as DSC and Intersection over union (IoU) ([see above](#dsc_hd_iou)). Thus combining related metrics will not change the ranking, and only metrics measuring different properties should be aggregated.\n",
    "<br>\n",
    "\n",
    "<div id='right_metric'></div>\n",
    "\n",
    "#### Choosing the right metric for a given task\n",
    "\n",
    "The selection of the most appropriate metric depends on your biomedical question and the characteristics of its problem: \n",
    "\n",
    "- What is the size, volume and shape of structures?\n",
    "- Are there image artefacts?\n",
    "- What is the annotation quality?\n",
    "- Is computation time relevant?\n",
    "- Is there any reference available?\n",
    "- Do we prefer higher sensitivity or specificity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/question_ico.png\" width=\"42\" height=\"42\"></div>\n",
    "\n",
    "### *Question 1*:\n",
    "\n",
    "Describe a situation where volume computation would be an appropriate criterion for measuring the quality of a segmentation task. When should it not be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Type your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/question_ico.png\" width=\"42\" height=\"42\"></div>\n",
    "\n",
    "### *Question 2*:\n",
    "What information about segmentation quality is revealed by the Hausdorff distance? Please describe a scenario where this measure is important to rate a segmentation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Type your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/question_ico.png\" width=\"42\" height=\"42\"></div>\n",
    "\n",
    "### *Question 3*:\n",
    "What needs to be made sure when selecting test data for ground truth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Type your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/question_ico.png\" width=\"42\" height=\"42\"></div>\n",
    "\n",
    "### *Question 4*:\n",
    "Why is it necessary to carry out manual segmentation several times by different and by the same person if it shall be used for ground truth? How is the information that is gained from these multiple segmentations used for rating the performance of an algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Type your answer here</font>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "7cf3cfb4d2a53586223bf4603cd7f9e645cf44a77dbcec96182c9a81e54296ad"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
